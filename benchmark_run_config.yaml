# Full empirical benchmark â€” CPU-tuned
global:
  device: "cpu"
  output_dir: "benchmark_results"
  training_steps: 10000
  eval_episodes: 100
  log_interval: 500
  eval_interval: 2500
  save_checkpoints: true

seeds: [1, 2, 3, 4, 5]

environment:
  num_channels: 10
  sequence_length: 16
  num_features: 3
  pu_on_prob: 0.3
  pu_off_prob: 0.5
  noise_std: 0.1
  max_steps: 200

agents:
  sac_lfm:
    class: "sac_lfm"
    params:
      model_dim: 64
      latent_dim: 64
      num_blocks: 2
      num_heads: 4
      lr: 3.0e-4
      gamma: 0.99
      tau: 0.005
      buffer_size: 100000
      batch_size: 128
      learning_starts: 500

  sac_lstm:
    class: "sac_lstm"
    params:
      hidden_dim: 64
      latent_dim: 64
      num_layers: 2
      lr: 3.0e-4
      gamma: 0.99
      tau: 0.005
      buffer_size: 100000
      batch_size: 128
      learning_starts: 500

  ppo_lstm:
    class: "ppo_lstm"
    params:
      lstm_hidden_size: 64
      n_lstm_layers: 1
      lr: 3.0e-4
      n_steps: 1024
      batch_size: 64
      n_epochs: 10
      gamma: 0.99
      gae_lambda: 0.95
      clip_range: 0.2
      ent_coef: 0.01
